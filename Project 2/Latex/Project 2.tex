\documentclass{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[fleqn]{amsmath}
\usepackage{titling}
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{amssymb}
\usepackage{listings}
\usepackage[font=small,labelsep=none]{caption}

\setlength{\droptitle}{-10em}

\title{Project 2}\vspace{-3ex}
\author{Benedicte Allum Pedersen, Emil Heland Broll\\ Fredrik Oftedal Forr}
\date{\vspace{-5ex}}

\begin{document}
\maketitle

In this project we will solve eigenpair-problems using numerical calculations.

We will start off by proving that orhogonal or unitary transformations preserves the orthogonality/dot product of the vectors.

Starting with an orthogonal basis of vectors, $\textbf{v}_i$, we know that:

\begin{flalign*}
  \begin{aligned}
	\textbf{v}_i = \begin{bmatrix}
	  v_{i1} \\
	  \vdots \\
	  v_{in}
	\end{bmatrix},
  \end{aligned}
  \qquad
  \begin{aligned}
	\textbf{v}_j^T \textbf{v}_i = \delta_{ij}
  \end{aligned}
\end{flalign*}

An orthogonal transformation gives us the following:
\begin{flalign*}
  &\textbf{w}_i=U\textbf{v}_i, \text{where } U^TU=UU^T=\textbf{I}
\end{flalign*}

We want to prove that orthogonality is preserved:
\begin{flalign*}
  \textbf{w}_i^T \textbf{w}_j = (U\textbf{v}_i)^T (U\textbf{v}_j) = \textbf{v}_i^T U^T U \textbf{v}_j = \textbf{v}_i^T \textbf{v}_j
\end{flalign*}

Now that we have proved that orthogonal transformations preserve orthogonality, we can move on to performing our Jacobi iterations. To do so, we use an orthogonal transformation matrix, S:
\begin{flalign*}
	S = \begin{bmatrix}
		1 & 0 & \cdots \\
		0 & 1 &  0 \\
		\vdots & 0 & \ddots \\
		& & & \cos{\theta} & 0 & \cdots & \sin{\theta} \\
		& & & 0 & 1 & \cdotsÂ & 0 \\
		& & & \vdots & \vdots & \ddots & \vdots \\
		& & & -\sin{\theta} & 0 & \cdots & \cos{\theta}
	\end{bmatrix}, \qquad S^T=S^{-1}
\end{flalign*}

We simpltfy by assigning:
\begin{flalign*}
	c = \cos{\theta}, s = \sin{\theta}, t = \tan{\theta} = \frac{s}{c}
\end{flalign*}

For our positive definite matrix ${A}$, we perform the transformation ${S^TA S = B}$, giving us the general expression:
\begin{flalign*}
&	b_{ii} = a_{ii}, i \neq k, i \neq l \\
&	b_{ik} = a_{ik}c - a_{il} s, i \neq k, i\neq l \\
&	b_{il} = a_{il} c + a_{ik} s, i \neq k, i\neq l \\
&	b_{kk} = a_{kk} c^2 - 2a_{kl}cs + a_{ll} s^2 \\
&	b_{ll} = a_{ll} c^2 - 2a_{kl}cs + a_{kk} s^2 \\
&	b_{kl} = (a_{kk}-a_{ll})cs + a_{kl}(c^2-s^2)
\end{flalign*}

By choosing the largest non-diagonal element in the original matrix $A$, we can fix $\theta$ by choosing to set the largest non-diagonal element to zero. From this, we can deduce the required values of $c$ and $s$:
\begin{flalign*}
	c = \frac{1}{\sqrt(1+t^2)}, \qquad s = tc \\
	t = -\tau \pm \sqrt{1+\tau^2}, \qquad \tau = \frac{a_{ll}-a_{kk}}{2a_{kl}}
\end{flalign*}

This will result in a new matrix, B. We can then find the largest non-diagonal element of the new matrix and repeat the algorithm until this value is less than a given tolerance.\\
We have tested multiple tolerances and adjusted the maximum iterations to make sure the algorithm gets to finish properly. A table of our findings is below.\\
Number of similarity iterations required to reach tolerance \\
Estimate of number of iterations as a func of matrix dimensionality.\\
TABLE HERE \\
Compared to Armadillos \textit{eigsys}-solver: ... \\
Time needed for running it...


\end{document}
